{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1a5c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import linalg as LA\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1)\n",
    "import csv\n",
    "import os\n",
    "import sklearn.metrics.pairwise as pdist\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc1a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"packages_headers.py\"\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from heapq import heapify, heappush, heappop\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse import csr_matrix\n",
    "#import scipy.sparse as sp\n",
    "import time\n",
    "t_s = time.time()\n",
    "#print(\"time passed:\"+ str(time.time()-t_s))\n",
    "import sklearn.metrics.pairwise as pdist\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import gc\n",
    "#del(variable_name)\n",
    "gc.collect()\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb85c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chenzhenxu/CodeProjects/jupyter_notebook/Arcdata\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea58d518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chenzhenxu/CodeProjects/jupyter_notebook/Arcdata/knn\n"
     ]
    }
   ],
   "source": [
    "datadr=cwd[:-4]+str(\"data/knn\")\n",
    "print(datadr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c98b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabDic(lables):\n",
    "    \"\"\"input a list of lables\n",
    "    return a dictionary mapping labels to 0,1,2,3...n-1\n",
    "    \"\"\"\n",
    "    Labs=list(set(lables))\n",
    "    n=len(Labs)\n",
    "    labdic={}\n",
    "    for i in range(n):\n",
    "        labdic[Labs[i]]=i\n",
    "    for k,v in labdic.items():\n",
    "        print(str(k)+\"--->\"+str(v))\n",
    "    return labdic\n",
    "\n",
    "def getdatafromFile(filenm):\n",
    "    \"\"\" this function uses the input filename\n",
    "      assume each line: label +\\t+ line\n",
    "      return list of labels, list of lines\n",
    "    \"\"\"\n",
    "    dat=[]\n",
    "    Y=[]\n",
    "    openf=open(filenm, 'r', encoding=\"latin1\")\n",
    "    for i, ln in enumerate(openf):\n",
    "        ln2=ln.split('\\t')\n",
    "        Y.append(ln2[0])\n",
    "        dat.append(ln2[1])\n",
    "    openf.close()\n",
    "    return (dat,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7f78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab44d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r8fn=datadr+'/r8-test-all-terms.txt'\n",
    "train_r8fn=datadr+'/r8-train-all-terms.txt'\n",
    "test_20ngfn=datadr+'/20ng-test-all-terms.txt'\n",
    "train_20ngfn=datadr+'/20ng-train-all-terms.txt'\n",
    "train_ohfn=datadr+'/train_ohsumed_by_line.txt'\n",
    "test_ohfn=datadr+'/test_ohsumed_by_line.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f549c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chenzhenxu/CodeProjects/jupyter_notebook/Arcdata/knn/r8-test-all-terms.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_r8fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d704af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crude--->0\n",
      "grain--->1\n",
      "money-fx--->2\n",
      "earn--->3\n",
      "trade--->4\n",
      "ship--->5\n",
      "interest--->6\n",
      "acq--->7\n"
     ]
    }
   ],
   "source": [
    "(traindat,trainY)=getdatafromFile(train_r8fn)\n",
    "(testdat, testY)=getdatafromFile(test_r8fn)\n",
    "r8Labdic=getLabDic(trainY)\n",
    "train_lab=[r8Labdic[e] for e in trainY]\n",
    "test_lab=[r8Labdic[e] for e in testY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fbbcf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5485\n",
      "2189\n"
     ]
    }
   ],
   "source": [
    "print(len(train_lab))\n",
    "print(len(test_lab))\n",
    "train_lab=np.asarray(train_lab)\n",
    "test_lab=np.asarray(test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e5006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r8LabdicRev={}\n",
    "for k,v in r8Labdic.items():\n",
    "    r8LabdicRev[v]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7338a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=['I like Brandeis', 'machine learning', 'I work at Brandeis','machine learning is awesome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f25f0647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 4 docs.\n",
      "(4, 8)\n",
      "length of vocab=8\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=None)\n",
    "N=len(docs)\n",
    "print(f\"there are { N } docs.\")\n",
    "tfx=vect.fit_transform(docs)\n",
    "print(tfx.shape)\n",
    "vocabs=vect.get_feature_names_out()\n",
    "print(f\"length of vocab={len(vocabs)}\")\n",
    "dfs=csr_matrix(((tfx>0)*1).sum(axis=0)).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "997b0366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 1, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eb3bb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['at', 'awesome', 'brandeis', 'is', 'learning', 'like', 'machine',\n",
       "       'work'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f89aee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfx.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afcd3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfx2=normalize(tfx, norm='l1',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1887fd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.5       , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
       "        0.        , 0.5       , 0.        ],\n",
       "       [0.33333333, 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.33333333],\n",
       "       [0.        , 0.25      , 0.        , 0.25      , 0.25      ,\n",
       "        0.        , 0.25      , 0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfx2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b229dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=csr_matrix(((tfx>0)*1).sum(axis=0)).toarray()[0] # document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cfee121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 1, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09239cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc58cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce8360c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(traindocs, stopwordlist=None):\n",
    "    vect = CountVectorizer(ngram_range=(1,1), stop_words=stopwordlist)\n",
    "    N=len(traindocs)\n",
    "    print(f\"there are { N } docs.\")\n",
    "    tfx=vect.fit_transform(traindocs)\n",
    "    print(tfx.shape)\n",
    "    vocabs=vect.get_feature_names_out()\n",
    "    print(f\"length of vocab={len(vocabs)}\")\n",
    "    dfs=csr_matrix(((tfx>0)*1).sum(axis=0)).toarray()[0]\n",
    "    print(\"shape of dfs:\"+str(dfs.shape))\n",
    "    print(\"len of dfs: \"+str(len(dfs)))\n",
    "\n",
    "    vocabs=vect.get_feature_names_out()\n",
    "    print(f\"length of vocab={len(vocabs)}\")\n",
    "    \n",
    "    ### tfx1 matrix indicating existence of a word or not (1/0) for tfx0 purposes\n",
    "    # tfx1=sp.csr_matrix((tfx>0)*1)\n",
    "    ## l1 normalize to frequency for each row/doc\n",
    "    #tfx=csr_matrix(tfx/tfx.sum(axis=1))\n",
    "\n",
    "    tfx=normalize(tfx, norm='l1',axis=1)\n",
    "    print(\"shape of tfx1: \"+str(tfx.shape))\n",
    "    print(\"computing tfidf ... \")\n",
    "    idfs=[1+np.log(N/(1+d)) for d in dfs]\n",
    "    idfs_diag=diags(idfs) \n",
    "    tfidf=tfx.dot(idfs_diag)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee8c0e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5485 docs.\n",
      "(5485, 19956)\n",
      "length of vocab=19956\n",
      "shape of dfs:(19956,)\n",
      "len of dfs: 19956\n",
      "length of vocab=19956\n",
      "shape of tfx1: (5485, 19956)\n",
      "computing tfidf ... \n"
     ]
    }
   ],
   "source": [
    "tfidf(traindat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58b9b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(testdocs, testlabs): # testdocs is a list of docs\n",
    "    predictionlabs=predict(testdocs)\n",
    "    #prediction=np.asarray(prediction)\n",
    "    count=0; ntest=len(predictionlabs)\n",
    "    for i in range(ntest):\n",
    "        if predictionlabs[i]==testlabs[i]:\n",
    "            count=count+1\n",
    "    test_error = 1 - count/ntest\n",
    "    return test_error\n",
    "\n",
    "\n",
    "def KNNpredict(neighbor_classes, C, cross_val=False):\n",
    "    # Make sure all classes are considered\n",
    "    labels = np.concatenate((neighbor_classes, list(range(C))))\n",
    "    # Find class frequency among neighbors\n",
    "    weights = np.unique(labels, return_counts=True)[1]\n",
    "    # Find most popular class\n",
    "    prediction = np.argmax(weights)\n",
    "    nn=len(neighbor_classes)\n",
    "    # If most popular class is ambiguous try with fewer neighbors; else return\n",
    "    if sum(weights[prediction] == weights) > 1 and nn>2:\n",
    "        return KNNpredict(neighbor_classes[:-2], C)\n",
    "    else:\n",
    "        return prediction\n",
    "\n",
    "\n",
    "\n",
    "def knnClassify(y_train, dist, n_neighbors=7): \n",
    "    ## y_train: training docs labels; dist: testdocs vs traindocs pairwise distance matrix \n",
    "    # Number of all different classes\n",
    "    #n_classes = len(np.unique(y_train))\n",
    "    prediction = []\n",
    "    ntestsample=dist.shape[0]\n",
    "    for i in range(ntestsample):\n",
    "        doc_to_train=dist[i,:]\n",
    "        # Find indices of n_neighbors closest documents\n",
    "        rank = np.argsort(doc_to_train)[:n_neighbors]\n",
    "        # Make prediction based on most popular class among neighbors\n",
    "        prediction.append(KNNpredict(y_train[rank], n_classes))\n",
    "    return prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
